{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import argparse as ap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_MODEL_PATH = \"..\\\\models\"\n",
    "DEFAULT_DATA_PATH = \"..\\\\csv\\\\data.csv\"\n",
    "GENERIC_MODEL_FILE_NAME = \"model_epoch_\"\n",
    "GENERIC_SCALAR_FILE_NAME = \"scalar_epoch_\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "modelRegexFilter = re.compile(GENERIC_MODEL_FILE_NAME + r\"(?P<epoch>\\d+)\")\n",
    "scalerRegexFilter = re.compile(GENERIC_SCALAR_FILE_NAME + r\"(?P<epoch>\\d+)\")\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.fc5 = nn.Linear(32, 16)\n",
    "        self.fc6 = nn.Linear(16, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "    \n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32).to(device)\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.float32).to(device)\n",
    "        self.inputs = torch.tensor(features, dtype=torch.float32).to(device)   \n",
    "        self.inputs = torch.tensor(features, dtype=torch.float32).to(device) \n",
    "          \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "def saveModel(model, scaler, path, epoch, fileLimit):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    torch.save(model.state_dict(), path + \"\\\\\"+ GENERIC_MODEL_FILE_NAME + str(epoch) + \".pt\")\n",
    "    with open(path+ \"\\\\\" + GENERIC_SCALAR_FILE_NAME+str(epoch) + \".pkl\", \"wb\") as file:\n",
    "        pickle.dump(scaler, file)\n",
    "    \n",
    "    files = os.listdir(path)\n",
    "    files = [f for f in files if os.path.isfile(path+'/'+f)]\n",
    "    if len(files) > 0:\n",
    "        modelEpochList = [int(modelRegexFilter.match(file).groups(\"epoch\")[0])  for file in files if modelRegexFilter.match(file)]\n",
    "        scalarEpochList = [int(scalerRegexFilter.match(file).groups(\"epoch\")[0])  for file in files if scalerRegexFilter.match(file)]\n",
    "        modelEpochList.sort(reverse=True)\n",
    "        scalarEpochList.sort(reverse=True)\n",
    "        for epoch in modelEpochList[fileLimit:]:\n",
    "            os.remove(path + \"\\\\\"+ GENERIC_MODEL_FILE_NAME + str(epoch) + \".pt\")\n",
    "            os.remove(path+ \"\\\\\" + GENERIC_SCALAR_FILE_NAME+str(epoch) + \".pkl\")\n",
    "\n",
    "\n",
    "\n",
    "def loadModel(path, input_size, num_classes):\n",
    "    model = Net(input_size, num_classes)\n",
    "    scaler = StandardScaler()\n",
    "    epoch = 0\n",
    "    if os.path.exists(path):\n",
    "        files = os.listdir(path)\n",
    "        files = [f for f in files if os.path.isfile(path+'/'+f)]\n",
    "        if len(files) > 0:\n",
    "            modelEpochList = [int(modelRegexFilter.match(file).groups(\"epoch\")[0])  for file in files if modelRegexFilter.match(file)]\n",
    "            scalarEpochList = [int(scalerRegexFilter.match(file).groups(\"epoch\")[0])  for file in files if scalerRegexFilter.match(file)]\n",
    "            modelEpochList.sort(reverse=True)\n",
    "            scalarEpochList.sort(reverse=True)\n",
    "            if modelEpochList[0] != scalarEpochList[0]:\n",
    "                for e in modelEpochList:\n",
    "                    if e in scalarEpochList:\n",
    "                        epoch = e\n",
    "            else:\n",
    "                epoch = modelEpochList[0]\n",
    "\n",
    "            \n",
    "            model.load_state_dict(torch.load(path +\"\\\\\"+ GENERIC_MODEL_FILE_NAME + str(epoch) + \".pt\"))\n",
    "            with open(path+ \"\\\\\" + GENERIC_SCALAR_FILE_NAME+str(epoch) + \".pkl\", \"rb\") as file:\n",
    "                scaler = pickle.load(file)\n",
    "\n",
    "    return model.to(device), scaler, epoch\n",
    "\n",
    "def readCSV(dataPath):\n",
    "    if os.path.exists(dataPath):\n",
    "        files = os.listdir(dataPath)\n",
    "        df = pd.DataFrame()\n",
    "        for file in files:\n",
    "            dfTemp = pd.read_csv(dataPath+file, sep=\",\")\n",
    "            df = pd.concat([df,dfTemp],ignore_index=True)\n",
    "            # print(dfTemp)\n",
    "        # print(df)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Angle   CurrentLapTime   Damage   DistanceFromStart  \\\n",
      "0 -1.748460e-07           -0.982      0.0             22125.8   \n",
      "1 -1.748460e-07           -0.962      0.0             22125.8   \n",
      "2 -1.748460e-07           -0.942      0.0             22125.8   \n",
      "3 -1.748460e-07           -0.922      0.0             22125.8   \n",
      "4 -1.748460e-07           -0.902      0.0             22125.8   \n",
      "\n",
      "    DistanceCovered   FuelLevel   Gear   LastLapTime   Opponent_1  Opponent_2  \\\n",
      "0               0.0     94.0000      0           0.0      30.0009       200.0   \n",
      "1               0.0     94.0000      0           0.0      30.0009       200.0   \n",
      "2               0.0     93.9999      0           0.0      30.0009       200.0   \n",
      "3               0.0     93.9998      0           0.0      30.0009       200.0   \n",
      "4               0.0     93.9997      0           0.0      30.0009       200.0   \n",
      "\n",
      "   ...   WheelSpinVelocity_1  WheelSpinVelocity_2  WheelSpinVelocity_3  \\\n",
      "0  ...                   0.0                  0.0                  0.0   \n",
      "1  ...                   0.0                  0.0                  0.0   \n",
      "2  ...                   0.0                  0.0                  0.0   \n",
      "3  ...                   0.0                  0.0                  0.0   \n",
      "4  ...                   0.0                  0.0                  0.0   \n",
      "\n",
      "   WheelSpinVelocity_4         Z   Acceleration  Braking  Clutch  Gear  \\\n",
      "0                  0.0  0.345261            1.0      0.0    0.64     1   \n",
      "1                  0.0  0.345261            1.0      0.0    0.64     1   \n",
      "2                  0.0  0.345261            1.0      0.0    0.64     1   \n",
      "3                  0.0  0.345261            1.0      0.0    0.64     1   \n",
      "4                  0.0  0.345261            1.0      0.0    0.64     1   \n",
      "\n",
      "   Steering  \n",
      "0 -0.028559  \n",
      "1 -0.028559  \n",
      "2 -0.028559  \n",
      "3 -0.028559  \n",
      "4 -0.028559  \n",
      "\n",
      "[5 rows x 79 columns]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a281edc2e540269214dee4f6cb2910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/200], Loss: 0.029194530329921028                    \r"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "modelName = \"INSERT_MODEL_NAME_HERE\"\n",
    "dataPath = '..\\\\csv\\\\INSERT_CSV_PATH_HERE\\\\'\n",
    "# dataPath = args.data_path\n",
    "DEFAULT_MODEL_PATH = \"..\\\\models\"\n",
    "modelPath = DEFAULT_MODEL_PATH\n",
    "amountOfEpochs = 10\n",
    "epochs = 200\n",
    "fileLimit = 5\n",
    "\n",
    "# Accuracy calculation\n",
    "def calculate_accuracy(true_labels, predictions, threshold=0.1):\n",
    "    correct = np.abs(true_labels - predictions) < threshold\n",
    "    accuracy = correct.mean() * 100\n",
    "    return accuracy\n",
    "\n",
    "# selectedFeatures = ['speedX', 'speedY','angle','trackPos', 'brake', 'accel', 'steer']\n",
    "selectedFeatures = [' SpeedX', ' SpeedY', 'Angle', 'TrackPosition', 'Braking', ' Acceleration', 'Steering', \n",
    "                    ' Track_1','Track_2','Track_3','Track_4','Track_5','Track_6','Track_7','Track_8','Track_9','Track_10','Track_11','Track_12','Track_13','Track_14','Track_15','Track_16','Track_17','Track_18','Track_19',\n",
    "                    ' SpeedZ', \n",
    "                    ' Opponent_1', 'Opponent_2', 'Opponent_3', 'Opponent_4', 'Opponent_5', 'Opponent_6', 'Opponent_7', 'Opponent_8', 'Opponent_9', 'Opponent_10',\n",
    "                    'Opponent_11', 'Opponent_12', 'Opponent_13', 'Opponent_14', 'Opponent_15', 'Opponent_16', 'Opponent_17', 'Opponent_18', 'Opponent_19', 'Opponent_20',\n",
    "                    'Opponent_21', 'Opponent_22', 'Opponent_23', 'Opponent_24', 'Opponent_25', 'Opponent_26', 'Opponent_27', 'Opponent_28', 'Opponent_29', 'Opponent_30',\n",
    "                    'Opponent_31', 'Opponent_32', 'Opponent_33', 'Opponent_34', 'Opponent_35', 'Opponent_36']\n",
    "\n",
    "# Load data\n",
    "df = readCSV(dataPath)\n",
    "# print(df)\n",
    "print(df.head())\n",
    "\n",
    "dfSelected = df.copy()\n",
    "dfSelected.dropna(axis=0)\n",
    "input = dfSelected[[' SpeedX', ' SpeedY', 'Angle', 'TrackPosition', \n",
    "                    ' Track_1','Track_2','Track_3','Track_4','Track_5','Track_6','Track_7','Track_8','Track_9','Track_10','Track_11','Track_12','Track_13','Track_14','Track_15','Track_16','Track_17','Track_18','Track_19' \n",
    "                    ,' SpeedZ',' Opponent_1', 'Opponent_2', 'Opponent_3', 'Opponent_4', 'Opponent_5', 'Opponent_6', 'Opponent_7', 'Opponent_8', 'Opponent_9', 'Opponent_10',\n",
    "                    'Opponent_11', 'Opponent_12', 'Opponent_13', 'Opponent_14', 'Opponent_15', 'Opponent_16', 'Opponent_17', 'Opponent_18', 'Opponent_19', 'Opponent_20',\n",
    "                    'Opponent_21', 'Opponent_22', 'Opponent_23', 'Opponent_24', 'Opponent_25', 'Opponent_26', 'Opponent_27', 'Opponent_28', 'Opponent_29', 'Opponent_30',\n",
    "                    'Opponent_31', 'Opponent_32', 'Opponent_33', 'Opponent_34', 'Opponent_35', 'Opponent_36']]\n",
    "                    \n",
    "output = dfSelected[['Braking', ' Acceleration', 'Steering']]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(input, output, test_size=0.2)\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "num_classes = len(set(output))\n",
    "\n",
    "net, scaler, oldEpoch = loadModel(modelPath + \"\\\\\" + modelName, input_size, num_classes)\n",
    "\n",
    "# # Standardize scaler features\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "#dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "criterion = nn.L1Loss()  # Mean Absolute Error\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr=0.1)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "loss_data = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    if epoch != 0 and epoch % amountOfEpochs == 0:\n",
    "        saveModel(net, scaler, modelPath + \"\\\\\" + modelName, epoch+oldEpoch, fileLimit)\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # Move inputs and labels to GPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # print(\"inputs\", inputs)\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "        # print(\"outputs\", outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        # print(loss.item())\n",
    "        # if i % 100 == 99:    # Print every 100 mini-batches\n",
    "        #     print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/100}')\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader)}                    ', end=\"\\r\")\n",
    "    loss_data.append(running_loss/len(train_loader))\n",
    "\n",
    "    # Evaluation\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "        test_losses.append(test_loss/len(test_loader))\n",
    "    \n",
    "print(\"\")\n",
    "print(test_losses)\n",
    "\n",
    "plt.plot(loss_data, label = \"training_loss\")\n",
    "plt.plot(test_losses, label = \"test_loss\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "saveModel(net, scaler, modelPath + \"\\\\\" + modelName, epoch+oldEpoch, fileLimit)\n",
    "\n",
    "# Evaluation\n",
    "# # input = dfSelected[['speedX', 'speedY','angle','trackPos']]\n",
    "# # input = [20.194000,291.326358,2.366310,0.044153,0.486511,0.000000,1.000033,-0.020732]\n",
    "# input = [[200, 2.366310, 0.044153, 1]]\n",
    "# column_names = ['speedX', 'speedY','angle','trackPos']\n",
    "# input_df = pd.DataFrame(input, columns=column_names)\n",
    "# input_df = scaler.transform(input_df)\n",
    "# input_tensor = torch.tensor(input_df, dtype=torch.float32).to(device)\n",
    "# print(\"input_tensor\", input_tensor)\n",
    "\n",
    "# Evaluation\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        predictions.extend(outputs.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "predictions = np.array(predictions)\n",
    "true_labels = np.array(true_labels)\n",
    "\n",
    "# Calculate MAE and RMSE\n",
    "mae = mean_absolute_error(true_labels, predictions)\n",
    "rmse = np.sqrt(mean_squared_error(true_labels, predictions))\n",
    "\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "\n",
    "accuracy = calculate_accuracy(true_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataPath = '..\\\\csv\\\\cg-track2\\\\'\n",
    "# readCSV(dataPath)\n",
    "\n",
    "# df = pd.read_csv(dataPath, sep=\",\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
